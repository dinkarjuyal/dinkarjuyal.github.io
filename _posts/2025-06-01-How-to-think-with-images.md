---
title: "How to think with images"
date: 2025-06-01
---

* TOC
{:toc}

### Introduction
"Think with images" - this is the promise of recent models like o3 from OpenAI, which can integrate images directly into their chain-of-thought, even zooming in or rotating an image as part of the reasoning process [1](https://www.theverge.com/news/649941/openai-o3-o4-mini-model-images-reasoning). The fact that it took this long to release such a capability hints that doing a reliable visual search in an image remains surprisingly hard. Why? In text, we deal with concepts already encoded into words, but an image is raw pixels – a dense, noisy form of signal. Two pictures of the same size might carry wildly different amounts of information. One could be a simple photo of a single object, while another could be a chaotic “Where’s Waldo?” scene filled with tiny characters.
Humans tackle this by zooming in and out, scanning for relevant clues. AI models, however, often struggle to localize and count things in crowded images. In fact, studies show that even state-of-the-art vision-language models (VLMs) strrugle at tasks trivial for humans – like deciding if two shapes overlap or counting simple objects [link](https://vlmsareblind.github.io/) . Their suboptimal performance on such visual acuity tests suggests that current AIs sometimes “see” like a myopic person, missing fine details. All this underscores a core challenge: visual understanding is high- bandwidth and context-dependent. An image doesn’t highlight the important bits for you – an AI has to figure that out on its own, potentially by thinking with images in a more human-like way.

![waldo](assets/how-to-think-with-images/where_waldo_bounding_boxes.jpg)
<p align="center">
Using o3 to predict bounding boxes for the prompt - "Identify people playing with red and white striped ball". In this case, the models is unable to localize the boxes correctly.
</p>

### Two Paths for Vision: Understanding vs. Generating
For a long time, computer vision followed two parallel paths. One focused on understanding images – identifying what’s in a photo and where it is. The other path tried having machines generate pictures from scratch. These two paths developed largely separately, but recently they are converging. To appreciate how we got here, let’s briefly recap some key advances in image generative models:

• __Variational Autoencoders (VAEs)__: VAEs were among the first modern generative models that learned to compress images into a latent code and then reconstruct them. They optimized a probabilistic lower bound, but often produced blurry outputs. A big issue was posterior collapse – the decoder would sometimes become too powerful and learn to ignore the latent representation. The result? The latent space carried little meaningful information.

• __VQ-VAE (Vector Quantized VAE)__: To fix that, VQ-VAEs discretize the latent space. The encoder outputs indices into a learned codebook of prototypical vectors instead of continuous codes. This simple change prevents the continuous optimization issues that caused posterior collapse 4 . Essentially, the model can’t cheat by blurring everything – it must choose discrete “tokens” to represent an image. Selecting the prototype vector for each smaller component in the latent space grid instead of the entire grid ensures diversity in generation - there is a combinatorial explosion in terms of getting the generated image. Paired with an autoregressive prior over these tokens, VQ-VAE greatly improved generation quality. It preserved important details better and enabled models that generated not just images but also audio and video with impressive fidelity.

• __VQ-GAN__: Building on VQ-VAE, the [VQ-GAN](https://arxiv.org/abs/2012.09841) introduced adversarial training and perceptual losses to sharpen the results. Rather than just minimizing pixel-wise differences (which can still produce fuzziness), a patch-based discriminator judges the reconstructed image quality. This pushes the generator to produce crisper, more realistic details. In short, VQ-GAN modified the VAE approach to match some of the realism that pure GANs (Generative Adversarial Networks) were known for, but without the instability of training a separate generator/discriminator from scratch for every new dataset.

• __Diffusion Models__: The next revolution came from a very different approach. [Diffusion models](https://arxiv.org/abs/2006.11239) generate images by iteratively denoising random noise, effectively learning to reverse a gradual noising process. During training, the model sees images with various levels of noise and learns to predict the noise added. Generation starts from pure noise and the model refines it step by step, “imagining” the picture as it erases noise. This process is slower than one-shot generation but astonishingly effective at producing high-quality, diverse images. Diffusion models don’t suffer mode-collapse and tend to capture fine details well. They have quickly become the state-of-the-art for text-to-image generation, as seen in tools like Stable Diffusion and DALL·E. One downside: working in pixel space is expensive – diffusion needed hundreds of steps on high-resolution images, which was a hurdle for speed.

• __Latent Diffusion & Flow Models__: To speed things up, researchers combined ideas. Latent diffusion compresses images with an autoencoder first (often a VAE) and then runs the diffusion process in the smaller [latent space](https://arxiv.org/abs/2112.10752). This two-step training (first train a VAE, then a diffusion model on its codes) preserves fidelity while cutting computation dramatically – one paper reported ~2.7× faster generation and 3× faster training by working in latent space. Meanwhile, [normalizing flows](https://arxiv.org/abs/1505.05770) offered another strategy: learn a single invertible transformation from noise to image (and vice versa) that gives exact likelihoods. Flows can be seen as a continuous generalization of the diffusion idea – in fact, you can view the diffusion reverse process as solving an ODE akin to a flow. In practice, flows (like Glow) achieved decent results but struggled to match diffusion in image realism, and they often required lots of parameters to capture complex image distributions. Still, flows introduced useful theoretical tools and insights for generative modeling.

![vq-gan](assets/how-to-think-with-images/vq_gan.png)
<p align="center">
VQ-GAN architecture - combine the efficiency of convolutional approaches with the expressiveness of transformers
</p>

Each of these innovations – from VQ-VAE to diffusion – was driven by a quest for better computational efficiency, training stability, and output quality. For example, latent VAE models avoided the posterior collapse problem of vanilla VAEs by using discrete codes 4 , and latent diffusion avoided the huge expense of pixel-level modeling by working on compressed representations. Most modern diffusion systems still train the image compressor (e.g. VAE) and the diffusion model separately. Recently, however, researchers have asked: why not train them end-to-end? A new training recipe called [REPA-E](https://arxiv.org/abs/2504.10483) does exactly that – it adds a “representation alignment” loss that lets the diffusion model and VAE learn together without one dominating the other. The result is faster training (a 45× speedup over the vanilla approach in one report!) and improved generative performance. In fact, end-to-end diffusion training with REPA-E not only sped things up ~17× relative to prior tuned methods, it even produced a better VAE the latent space became more structured, leading to a new state-of-the-art FID (image quality score) on ImageNet. It’s a great reminder that joint training can unlock capabilities that separate training misses.

__Contrastive Language-Image Pretraining__ : While generative models were evolving, another thread of research tackled the understanding side of vision by directly linking images with language. A landmark was OpenAI’s [CLIP](https://arxiv.org/abs/2103.00020]) . CLIP learned by simply predicting which caption goes with which image, across hundreds of millions of random image-text pairs from the web. This contrastive learning forces an image encoder and a text encoder to meet in the middle – to produce embeddings that match for a true pair and differ for a mismatched pair. CLIP’s latent space turned out to be remarkably semantically rich. All sorts of concepts (from objects to styles to famous faces) are encoded in a way that text and image descriptions align. This means we can do zero-shot classification: to recognize dogs vs. cats, we don’t fine-tune on thousands of labeled photos – we just embed the image and the text “a photo of a dog” (or “...cat”) and see which is closer in CLIP space. CLIP essentially taught models to read the room in an image using the supervision already contained in alt-text, captions, and the like.

__SigLIP__ : CLIP wasn’t alone for long. Variations soon appeared, such as Google’s [SigLIP](https://arxiv.org/abs/2303.15343) (“Sigmoid” CLIP), which tweaked the training loss. Instead of comparing all image-text pairs globally in a big softmax, SigLIP uses a pairwise sigmoid loss. This removes the need for extremely large batch sizes and negative sampling across the whole batch. In practice, SigLIP was found to outperform CLIP under smaller batch training regimes while still reaching similar performance at scale. It’s basically a more efficient way to train an image-text bi-encoder, addressing some engineering headaches of CLIP (which ideally wanted huge batches like 32k or more).

__BLIP__ : Another line of work, [BLIP](https://arxiv.org/abs/2201.12086) (Bootstrapping Language-Image Pretraining), took a slightly different approach. Instead of just learning “this image matches this caption”, BLIP models also learned to generate captions and even perform VQA (visual question answering) in a unified framework. The original BLIP introduced a captioning model that could filter out noisy data – it bootstrapped itself by generating captions and then training on those refined pairs. The result is a system that can accept an image and a question and then output a fluent answer, or produce a description, etc. Unlike CLIP’s single embedding space, BLIP has a generative head: it doesn’t just tell you which text is closest; it can actually speak about the image. This makes BLIP more effective on tasks requiring nuanced understanding and reasoning (e.g. writing a caption that truly reflects an image’s details or answering questions that require combining observations) – things that a pure embedding model like CLIP might miss. In essence, CLIP vs. BLIP highlights a difference in representation: CLIP’s embeddings are like conceptual keywords (great for retrieval and recognition), whereas BLIP’s model (with its language generation capability) carries more of the context and syntax, making it better at composing sentences about an image. Neither is “better” universally; they’re optimized for different uses, but together they point toward increasingly general vision-language understanding.

### Unifying Vision – Are We There Yet?

Having separate specialized models – one to generate images, another to caption them, another to answer questions – is not very elegant. The dream is a single AI that can see and generate, answer and ask, all in one system. How to get there? Two broad strategies have emerged: unify by fusion (build one model that does everything) or unify by coupling (connect a vision-understanding model to a vision-generating model).

On the “fusion” front, one trend is to treat everything as a sequence of tokens and train one big transformer to predict the next token – whether that token comes from text or from an image. Can we extend the success of autoregressive transformers (like GPT) to vision, by converting images into a language-like form? [LVM](https://yutongbai.com/lvm.html) (Large Vision Model) is a striking example. It represents images (and even videos, plus optional annotations like segmentation masks) as sequences of discrete tokens – essentially turning pixels into a stream of “visual words”. Then it trains a transformer to predict the next token on a massive corpus of these visual sentences (420 billion tokens). Remarkably, this purely vision-trained model can handle many tasks by simply being prompted with the right visual context. For instance, to classify an image, you might feed the model an image of a dog followed by a special “[CLASS]” token and see what label token it predicts next – akin to asking it, in visual terms, “what is this?” The authors report that with the right “visual prompts,” a single LVM can do segmentation, object detection, depth prediction, and more. This hints that a sufficiently large sequence model could learn a kind of general visual reasoning, parallel to how GPT-4 exhibits general language reasoning. The upside of this approach is unification through format – everything becomes a token sequence, so in theory one model handles all. The downside is it may require enormous data and compute, and it’s not yet clear if such models truly reason or just cleverly memorize visual patterns.

![lvm](assets/how-to-think-with-images/lvm.png)
<p align="center">
Large Vision Model - Each row represents a prompt composed of a sequence of images interleaved with annotations, ending in a query. Model predicts the final column.
</p>

Another attempt at unification is more hybrid: use one model to cover vision understanding and generation by bolting together the strengths of each. An example from 2025 is the [MetaQueries](https://arxiv.org/abs/2504.06256) approach. Instead of training one monolithic model from scratch, MetaQueries proposes a neat interface to connect an autoregressive multimodal LLM with a diffusion image generator. It introduces a set of learned query vectors that take the rich latent features from the language model (which has “understood” the user’s request and visual context) and feed them into the diffusion model’s decoder to produce an image. MetaQueries can train this connection with relatively standard image-caption pairs and diffusion objectives. Crucially, you don’t need to fine-tune the large language model – it can be frozen. This avoids degrading the language model’s knowledge, while still leveraging it to guide image generation. Early results showed that this combo could perform “knowledge-augmented” generation – for instance, ask the system to draw an image of a historical event, and it can use the language model’s knowledge to get details right. And because the interface is learned, it outperforms simpler methods like prompt engineering. 

![metaqueries](assets/how-to-think-with-images/metaqueries.png)
<p align="center">
Metaqueries are a set of learnable queries that connect the MLLM’s latents to the diffusion decoder
</p>

Between the extremes of one-model-for-everything and two-models-coupled, there’s a lot of room. Some methods try to merge models but still keep modularity inside. For example, the [SEED-X](https://arxiv.org/abs/2404.14396) system aims for a “unified multi-granularity” vision model – one that can both comprehend and generate, and do so at various levels of detail. SEED-X emphasizes handling arbitrarily sized images (which is vital for practical applications) and multi-scale generation. Under the hood, it still uses separate modules (it builds on a previous SEED-LLaMA architecture), but the integration is tight enough that, from the user’s perspective, you have one coherent model that will both interpret your input images and also produce new images on demand.
