---
title: "How to think with images"
date: 2025-06-01
---

* TOC
{:toc}

"Think with images" - this is the promise of recent models like o3 from OpenAI, which can integrate images directly into their chain-of-thought, even zooming in or rotating an image as part of the reasoning process [1](https://www.theverge.com/news/649941/openai-o3-o4-mini-model-images-reasoning). The fact that it took this long to release such a capability hints that doing a reliable visual search in an image remains surprisingly hard. Why? In text, we deal with concepts already encoded into words, but an image is raw pixels – a dense, noisy form of signal. Two pictures of the same size might carry wildly different amounts of information. One could be a simple photo of a single object, while another could be a chaotic “Where’s Waldo?” scene filled with tiny characters.
Humans tackle this by zooming in and out, scanning for relevant clues. AI models, however, often struggle to localize and count things in crowded images. In fact, studies show that even state-of-the-art vision-language models (VLMs) strrugle at tasks trivial for humans – like deciding if two shapes overlap or counting simple objects [link](https://vlmsareblind.github.io/) . Their suboptimal performance on such visual acuity tests suggests that current AIs sometimes “see” like a myopic person, missing fine details. All this underscores a core challenge: visual understanding is high- bandwidth and context-dependent. An image doesn’t highlight the important bits for you – an AI has to figure that out on its own, potentially by thinking with images in a more human-like way.

![waldo](assets/how-to-think-with-images/where_waldo_bounding_boxes.jpg)
<p align="center">
Using o3 to predict bounding boxes for the prompt - "identify people playing with red and white striped ball"
</p>

### Two Paths for Vision: Understanding vs. Generating
For a long time, computer vision followed two parallel paths. One focused on understanding images – identifying what’s in a photo and where it is. The other path tried having machines generate pictures from scratch. These two paths developed largely separately, but recently they are converging. To appreciate how we got here, let’s briefly recap some key advances in image generative models:
• Variational Autoencoders (VAEs): VAEs were among the first modern generative models that learned to compress images into a latent code and then reconstruct them. They optimized a probabilistic lower bound, but often produced blurry outputs. A big issue was posterior collapse – the decoder would sometimes become too powerful and learn to ignore the latent representation. The result? The latent space carried little meaningful information.
• VQ-VAE (Vector Quantized VAE): To fix that, VQ-VAEs discretize the latent space. The encoder outputs indices into a learned codebook of prototypical vectors instead of continuous codes. This simple change prevents the continuous optimization issues that caused posterior collapse 4 . Essentially, the model can’t cheat by blurring everything – it must choose discrete “tokens” to represent an image. Selecting the prototype vector for each smaller component in the latent space grid instead of the entire grid ensures diversity in generation - there is a combinatorial explosion in terms of getting the generated image. Paired with an autoregressive prior over these tokens, VQ-VAE greatly improved generation quality. It preserved important details better and enabled models that generated not just images but also audio and video with impressive fidelity.
• VQ-GAN: Building on VQ-VAE, the [VQ-GAN](https://arxiv.org/abs/2012.09841) introduced adversarial training and perceptual losses to sharpen the results. Rather than just minimizing pixel-wise differences (which can still produce fuzziness), a patch-based discriminator judges the reconstructed image quality. This pushes the generator to produce crisper, more realistic details. In short, VQ-GAN modified the VAE approach to match some of the realism that pure GANs (Generative Adversarial Networks) were known for, but without the instability of training a separate generator/discriminator from scratch for every new dataset.
• Diffusion Models: The next revolution came from a very different approach. [Diffusion models](https://arxiv.org/abs/2006.11239) generate images by iteratively denoising random noise, effectively learning to reverse a gradual noising process. During training, the model sees images with various levels of noise and learns to predict the noise added. Generation starts from pure noise and the model refines it step by step, “imagining” the picture as it erases noise. This process is slower than one-shot generation but astonishingly effective at producing high-quality, diverse images. Diffusion models don’t suffer mode-collapse and tend to capture fine details well. They have quickly become the state-of-the-art for text-to-image generation, as seen in tools like Stable Diffusion and DALL·E. One downside: working in pixel space is expensive – diffusion needed hundreds of steps on high-resolution images, which was a hurdle for speed.
• Latent Diffusion & Flow Models: To speed things up, researchers combined ideas. Latent diffusion compresses images with an autoencoder first (often a VAE) and then runs the diffusion process in the smaller [latent space](https://arxiv.org/abs/2112.10752). This two-step training (first train a VAE, then a diffusion model on its codes) preserves fidelity while cutting computation dramatically – one paper reported ~2.7× faster generation and 3× faster training by working in latent space. Meanwhile, [normalizing flows](https://arxiv.org/abs/1505.05770) offered another strategy: learn a single invertible transformation from noise to image (and vice versa) that gives exact likelihoods. Flows can be seen as a continuous generalization of the diffusion idea – in fact, you can view the diffusion reverse process as solving an ODE akin to a flow. In practice, flows (like Glow) achieved decent results but struggled to match diffusion in image realism, and they often required lots of parameters to capture complex image distributions. Still, flows introduced useful theoretical tools and insights for generative modeling.

![vq-gan](assets/how-to-think-with-images/vq_gan.png)
<p align="center">
VQ-GAN architecture
</p>
Each of these innovations – from VQ-VAE to diffusion – was driven by a quest for better computational efficiency, training stability, and output quality. For example, latent VAE models avoided the posterior collapse problem of vanilla VAEs by using discrete codes 4 , and latent diffusion avoided the huge expense of pixel-level modeling by working on compressed representations 7 . Most modern diffusion systems still train the image compressor (e.g. VAE) and the diffusion model separately. Recently, however, researchers have asked: why not train them end-to-end? A new training recipe called [REPA-E](https://arxiv.org/abs/2504.10483) does exactly that – it adds a “representation alignment” loss that lets the diffusion model and VAE learn together without one dominating the other. The result is faster training (a 45× speedup over the vanilla approach in one report!) and improved generative performance. In fact, end-to-end diffusion training with REPA-E not only sped things up ~17× relative to prior tuned methods, it even produced a better VAE the latent space became more structured, leading to a new state-of-the-art FID (image quality score) on ImageNet. It’s a great reminder that joint training can unlock capabilities that separate training misses.

While generative models were evolving, another thread of research tackled the understanding side of vision by directly linking images with language. A landmark was OpenAI’s [CLIP](https://arxiv.org/abs/2103.00020])(Contrastive Language- Image Pretraining). CLIP learned by simply predicting which caption goes with which image, across hundreds of millions of random image-text pairs from the web. This contrastive learning forces an image encoder and a text encoder to meet in the middle – to produce embeddings that match for a true pair and differ for a mismatched pair. CLIP’s latent space turned out to be remarkably semantically rich. All sorts of concepts (from objects to styles to famous faces) are encoded in a way that text and image descriptions align. This means we can do zero-shot classification: to recognize dogs vs. cats, we don’t fine-tune on thousands of labeled photos – we just embed the image and the text “a photo of a dog” (or “...cat”) and see which is closer in CLIP space. CLIP essentially taught models to read the room in an image using the supervision already contained in alt-text, captions, and the like.

CLIP wasn’t alone for long. Variations soon appeared, such as Google’s [SigLIP](https://arxiv.org/abs/2303.15343) (“Sigmoid” CLIP), which tweaked the training loss. Instead of comparing all image-text pairs globally in a big softmax, SigLIP uses a pairwise sigmoid loss. This removes the need for extremely large batch sizes and negative sampling across the whole batch. In practice, SigLIP was found to outperform CLIP under smaller batch training regimes while still reaching similar performance at scale. It’s basically a more efficient way to train an image-text bi-encoder, addressing some engineering headaches of CLIP (which ideally wanted huge batches like 32k or more).

Another line of work, [BLIP](https://arxiv.org/abs/2201.12086) (Bootstrapping Language-Image Pretraining), took a slightly different approach. Instead of just learning “this image matches this caption”, BLIP models also learned to generate captions and even perform VQA (visual question answering) in a unified framework. The original BLIP introduced a captioning model that could filter out noisy data – it bootstrapped itself by generating captions and then training on those refined pairs. The result is a system that can accept an image and a question and then output a fluent answer, or produce a description, etc. Unlike CLIP’s single embedding space, BLIP has a generative head: it doesn’t just tell you which text is closest; it can actually speak about the image. This makes BLIP more effective on tasks requiring nuanced understanding and reasoning (e.g. writing a caption that truly reflects an image’s details or answering questions that require combining observations) – things that a pure embedding model like CLIP might miss. In essence, CLIP vs. BLIP highlights a difference in representation: CLIP’s embeddings are like conceptual keywords (great for retrieval and recognition), whereas BLIP’s model (with its language generation capability) carries more of the context and syntax, making it better at composing sentences about an image. Neither is “better” universally; they’re optimized for different uses, but together they point toward increasingly general vision-language understanding.

### Unifying Vision – Are We There Yet?

